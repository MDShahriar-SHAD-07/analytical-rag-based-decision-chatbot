{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.12"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14798432,"sourceType":"datasetVersion","datasetId":9461926},{"sourceId":14830169,"sourceType":"datasetVersion","datasetId":9484740}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ü§ñ Restaurant Intelligence Chatbot - Production System\n\n**Enterprise-Grade Conversational AI with RAG**\n\nThis system integrates:\n- ‚úÖ Advanced Sentiment Analysis\n- ‚úÖ Independent Aspect-Based Analysis\n- ‚úÖ Vector Retrieval (RAG)\n- ‚úÖ LLM-Driven Recommendations\n- ‚úÖ Hallucination Prevention\n- ‚úÖ Production-Grade Error Handling\n\n---\n\n## System Architecture\n\n```\nUser Query ‚Üí Intent Router ‚Üí Vector Retrieval ‚Üí LLM Reasoning ‚Üí Structured Response\n                ‚Üì                    ‚Üì                ‚Üì\n         Context Memory      Semantic Search    Grounded Generation\n```","metadata":{}},{"cell_type":"code","source":"!pip install -q \\\n    chromadb \\\n    langchain \\\n    langchain-community \\\n    sentence-transformers \\\n    transformers \\\n    accelerate \\\n    python-dotenv\n\n","metadata":{"execution":{"iopub.status.busy":"2026-02-15T15:58:24.914892Z","iopub.execute_input":"2026-02-15T15:58:24.915415Z","iopub.status.idle":"2026-02-15T15:58:41.583722Z","shell.execute_reply.started":"2026-02-15T15:58:24.915385Z","shell.execute_reply":"2026-02-15T15:58:41.583009Z"},"trusted":true},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m52.0/52.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m21.4/21.4 MB\u001b[0m \u001b[31m87.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m95.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m79.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m74.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m132.6/132.6 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m220.0/220.0 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m66.5/66.5 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m517.7/517.7 kB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m108.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m456.8/456.8 kB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-adk 1.22.1 requires google-cloud-bigquery-storage>=2.0.0, which is not installed.\nbigframes 2.26.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ngoogle-adk 1.22.1 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.39.1 which is incompatible.\ngoogle-adk 1.22.1 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\nopentelemetry-exporter-gcp-logging 1.11.0a0 requires opentelemetry-sdk<1.39.0,>=1.35.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.47.0 which is incompatible.\ngoogle-colab 1.0.0 requires jupyter-server==2.14.0, but you have jupyter-server 2.12.5 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\ncudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ngradio 5.49.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.5 which is incompatible.\nbigframes 2.26.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nopentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.39.1 which is incompatible.\nopentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.39.1 which is incompatible.\nopentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\nfastai 2.8.4 requires fastcore<1.9,>=1.8.0, but you have fastcore 1.11.3 which is incompatible.\npylibcudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## üì¶ Installation & Dependencies","metadata":{}},{"cell_type":"markdown","source":"## ‚öôÔ∏è Configuration & Environment Setup","metadata":{}},{"cell_type":"code","source":"# Core Imports\nimport os\nimport warnings\nimport logging\nfrom pathlib import Path\nfrom typing import Dict, List, Optional, Tuple, Any\nfrom dataclasses import dataclass, field\nimport json\n\n# Data Processing\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\n\n# ML & NLP\nimport torch\nfrom transformers import pipeline\nfrom sentence_transformers import SentenceTransformer\n\n# Vector DB & RAG\nimport chromadb\nfrom chromadb.config import Settings\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.vectorstores import Chroma\nfrom langchain.schema import Document\nfrom langchain.chains import RetrievalQA\nfrom langchain.prompts import PromptTemplate\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain.llms.base import LLM\n\n# Validation\nfrom pydantic import BaseModel, Field, validator\n\n# Configuration\nwarnings.filterwarnings('ignore')\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\ntqdm.pandas()\n\nprint(\"‚úÖ All dependencies loaded successfully\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T15:58:48.205022Z","iopub.execute_input":"2026-02-15T15:58:48.205770Z","iopub.status.idle":"2026-02-15T15:59:26.453278Z","shell.execute_reply.started":"2026-02-15T15:58:48.205734Z","shell.execute_reply":"2026-02-15T15:59:26.452526Z"}},"outputs":[{"name":"stderr","text":"2026-02-15 15:59:00.048478: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1771171140.275139      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1771171140.340010      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1771171140.873177      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1771171140.873230      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1771171140.873233      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1771171140.873235      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"name":"stdout","text":"‚úÖ All dependencies loaded successfully\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"@dataclass\nclass SystemConfig:\n    \"\"\"Production-grade configuration with environment variable support\"\"\"\n    \n    # Paths\n    data_path: str = \"/kaggle/input/datasets/shahriard07/restaurant-review/dhaka_restaurants.csv\"\n    vector_db_path: str = \"/kaggle/working/restaurant_vector_db\"\n    \n    # Model Configuration\n    sentiment_model: str = \"cardiffnlp/twitter-xlm-roberta-base-sentiment\"\n    embedding_model: str = \"all-MiniLM-L6-v2\"\n    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    \n    # LLM Configuration\n    llm_temperature: float = 0.2\n    max_tokens: int = 512\n    \n    # RAG Configuration\n    retrieval_k: int = 5\n    similarity_threshold: float = 0.7\n    \n    # Aspect Keywords\n    aspects: Dict[str, List[str]] = field(default_factory=lambda: {\n        \"food\": [\"food\", \"taste\", \"meal\", \"dish\", \"cuisine\", \"flavor\", \"delicious\", \"‡¶ñ‡¶æ‡¶¨‡¶æ‡¶∞\", \"‡¶∏‡ßç‡¶¨‡¶æ‡¶¶\"],\n        \"service\": [\"service\", \"staff\", \"waiter\", \"waitress\", \"manager\", \"server\", \"‡¶∏‡¶æ‡¶∞‡ßç‡¶≠‡¶ø‡¶∏\", \"‡¶ï‡¶∞‡ßç‡¶Æ‡ßÄ\"],\n        \"price\": [\"price\", \"cost\", \"expensive\", \"cheap\", \"value\", \"affordable\", \"‡¶¶‡¶æ‡¶Æ\", \"‡¶Æ‡ßÇ‡¶≤‡ßç‡¶Ø\"],\n        \"ambience\": [\"ambience\", \"atmosphere\", \"environment\", \"decor\", \"vibe\", \"‡¶™‡¶∞‡¶ø‡¶¨‡ßá‡¶∂\"],\n        \"cleanliness\": [\"clean\", \"hygiene\", \"sanitary\", \"dirty\", \"‡¶™‡¶∞‡¶ø‡¶∑‡ßç‡¶ï‡¶æ‡¶∞\"]\n    })\n    \n    # Negative Triggers\n    negative_triggers: List[str] = field(default_factory=lambda: [\n        \"late\", \"slow\", \"rude\", \"bad\", \"cold\", \"delay\", \"terrible\", \"awful\",\n        \"disappointing\", \"poor\", \"worst\", \"‡¶ñ‡¶æ‡¶∞‡¶æ‡¶™\", \"‡¶¶‡ßá‡¶∞‡¶ø\", \"‡¶†‡¶æ‡¶®‡ßç‡¶°‡¶æ\"\n    ])\n    \n    # Production Settings\n    batch_size: int = 32\n    enable_logging: bool = True\n    fallback_enabled: bool = True\n\n# Initialize configuration\nconfig = SystemConfig()\nlogger.info(f\"System initialized on device: {config.device}\")\nprint(f\"üîß Configuration loaded - Device: {config.device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T16:00:18.556075Z","iopub.execute_input":"2026-02-15T16:00:18.556932Z","iopub.status.idle":"2026-02-15T16:00:18.565534Z","shell.execute_reply.started":"2026-02-15T16:00:18.556903Z","shell.execute_reply":"2026-02-15T16:00:18.564880Z"}},"outputs":[{"name":"stdout","text":"üîß Configuration loaded - Device: cuda\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## üìä Data Layer - Schema Validation & Cleaning","metadata":{}},{"cell_type":"code","source":"class ReviewSchema(BaseModel):\n    \"\"\"Pydantic schema for review validation\"\"\"\n    business_name: str = Field(..., min_length=1)\n    review_text: str = Field(..., min_length=10)\n    review_rating: float = Field(..., ge=1.0, le=5.0)\n    business_address: Optional[str] = None\n    \n    @validator('review_text')\n    def validate_text(cls, v):\n        if not isinstance(v, str) or len(v.strip()) < 10:\n            raise ValueError('Review text must be at least 10 characters')\n        return v.strip()\n\nclass DataPipeline:\n    \"\"\"Production-grade data processing pipeline\"\"\"\n    \n    def __init__(self, config: SystemConfig):\n        self.config = config\n        self.df_raw = None\n        self.df_cleaned = None\n        \n    def load_and_validate(self, path: str) -> pd.DataFrame:\n        \"\"\"Load data with schema validation\"\"\"\n        try:\n            logger.info(f\"Loading data from {path}\")\n            df = pd.read_csv(path)\n            self.df_raw = df.copy()\n            \n            # Validate required columns\n            required_cols = ['business_name', 'review_text', 'review_rating']\n            missing = set(required_cols) - set(df.columns)\n            if missing:\n                raise ValueError(f\"Missing required columns: {missing}\")\n            \n            logger.info(f\"‚úÖ Loaded {len(df)} rows\")\n            return df\n            \n        except Exception as e:\n            logger.error(f\"Data loading failed: {e}\")\n            raise\n    \n    def clean_data(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Clean and normalize data\"\"\"\n        logger.info(\"Starting data cleaning pipeline\")\n        \n        # Drop nulls\n        df_clean = df.dropna(subset=['review_text', 'business_name']).copy()\n        logger.info(f\"Removed {len(df) - len(df_clean)} null rows\")\n        \n        # Ensure text is string\n        df_clean = df_clean[df_clean['review_text'].apply(lambda x: isinstance(x, str))]\n        \n        # Text normalization\n        df_clean['review_text'] = (\n            df_clean['review_text']\n            .str.strip()\n            .str.replace(r'\\s+', ' ', regex=True)\n        )\n        \n        # Filter short reviews\n        df_clean = df_clean[df_clean['review_text'].str.len() >= 10]\n        \n        # Normalize restaurant names\n        df_clean['business_name_normalized'] = (\n            df_clean['business_name']\n            .str.strip()\n            .str.lower()\n        )\n        \n        # Deduplicate\n        before_dedup = len(df_clean)\n        df_clean = df_clean.drop_duplicates(subset=['business_name', 'review_text'])\n        logger.info(f\"Removed {before_dedup - len(df_clean)} duplicate reviews\")\n        \n        self.df_cleaned = df_clean.reset_index(drop=True)\n        logger.info(f\"‚úÖ Cleaning complete - {len(self.df_cleaned)} clean rows\")\n        \n        return self.df_cleaned\n    \n    def get_restaurant_index(self) -> Dict[str, int]:\n        \"\"\"Create restaurant name index for fast lookup\"\"\"\n        if self.df_cleaned is None:\n            raise ValueError(\"Data not cleaned yet\")\n        \n        return (\n            self.df_cleaned\n            .groupby('business_name_normalized')\n            .size()\n            .to_dict()\n        )\n\n# Initialize and run data pipeline\ndata_pipeline = DataPipeline(config)\ndf = data_pipeline.load_and_validate(config.data_path)\ndf_cleaned = data_pipeline.clean_data(df)\n\nprint(f\"\\nüìä Data Summary:\")\nprint(f\"Total Reviews: {len(df_cleaned):,}\")\nprint(f\"Unique Restaurants: {df_cleaned['business_name'].nunique():,}\")\nprint(f\"Average Review Length: {df_cleaned['review_text'].str.len().mean():.0f} chars\")\ndf_cleaned.head(3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T16:00:22.737380Z","iopub.execute_input":"2026-02-15T16:00:22.737683Z","iopub.status.idle":"2026-02-15T16:00:22.858838Z","shell.execute_reply.started":"2026-02-15T16:00:22.737657Z","shell.execute_reply":"2026-02-15T16:00:22.858196Z"}},"outputs":[{"name":"stdout","text":"\nüìä Data Summary:\nTotal Reviews: 977\nUnique Restaurants: 126\nAverage Review Length: 405 chars\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"            business_name                            business_address  \\\n0  Izumi Japanese Kitchen  House 24 C, Rd 119, Dhaka 1212, Bangladesh   \n1  Izumi Japanese Kitchen  House 24 C, Rd 119, Dhaka 1212, Bangladesh   \n2  Izumi Japanese Kitchen  House 24 C, Rd 119, Dhaka 1212, Bangladesh   \n\n     business_phone                 business_website  business_rating  \\\n0  +880 1933-446677  https://m.facebook.com/izumiBD/              4.5   \n1  +880 1933-446677  https://m.facebook.com/izumiBD/              4.5   \n2  +880 1933-446677  https://m.facebook.com/izumiBD/              4.5   \n\n   business_total_reviews                                      reviewer_name  \\\n0                    2233  {'name': 'Raunak Maskay', 'thumbnail': 'https:...   \n1                    2233  {'name': 'Dewan Asif', 'thumbnail': 'https://l...   \n2                    2233  {'name': 'Dr. Mehruba Mona', 'thumbnail': 'htt...   \n\n   review_rating          review_date  \\\n0            5.0          a month ago   \n1            5.0         4 months ago   \n2            5.0  Edited 8 months ago   \n\n                                         review_text  review_additional_info  \\\n0  Izumi Japanese Kitchen in Gulshan, Dhaka is on...                     NaN   \n1  Izumi Japanese Kitchen is a great place for re...                     NaN   \n2  One of the authentic Japanese restaurant in Dh...                     NaN   \n\n  business_name_normalized  \n0   izumi japanese kitchen  \n1   izumi japanese kitchen  \n2   izumi japanese kitchen  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>business_name</th>\n      <th>business_address</th>\n      <th>business_phone</th>\n      <th>business_website</th>\n      <th>business_rating</th>\n      <th>business_total_reviews</th>\n      <th>reviewer_name</th>\n      <th>review_rating</th>\n      <th>review_date</th>\n      <th>review_text</th>\n      <th>review_additional_info</th>\n      <th>business_name_normalized</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Izumi Japanese Kitchen</td>\n      <td>House 24 C, Rd 119, Dhaka 1212, Bangladesh</td>\n      <td>+880 1933-446677</td>\n      <td>https://m.facebook.com/izumiBD/</td>\n      <td>4.5</td>\n      <td>2233</td>\n      <td>{'name': 'Raunak Maskay', 'thumbnail': 'https:...</td>\n      <td>5.0</td>\n      <td>a month ago</td>\n      <td>Izumi Japanese Kitchen in Gulshan, Dhaka is on...</td>\n      <td>NaN</td>\n      <td>izumi japanese kitchen</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Izumi Japanese Kitchen</td>\n      <td>House 24 C, Rd 119, Dhaka 1212, Bangladesh</td>\n      <td>+880 1933-446677</td>\n      <td>https://m.facebook.com/izumiBD/</td>\n      <td>4.5</td>\n      <td>2233</td>\n      <td>{'name': 'Dewan Asif', 'thumbnail': 'https://l...</td>\n      <td>5.0</td>\n      <td>4 months ago</td>\n      <td>Izumi Japanese Kitchen is a great place for re...</td>\n      <td>NaN</td>\n      <td>izumi japanese kitchen</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Izumi Japanese Kitchen</td>\n      <td>House 24 C, Rd 119, Dhaka 1212, Bangladesh</td>\n      <td>+880 1933-446677</td>\n      <td>https://m.facebook.com/izumiBD/</td>\n      <td>4.5</td>\n      <td>2233</td>\n      <td>{'name': 'Dr. Mehruba Mona', 'thumbnail': 'htt...</td>\n      <td>5.0</td>\n      <td>Edited 8 months ago</td>\n      <td>One of the authentic Japanese restaurant in Dh...</td>\n      <td>NaN</td>\n      <td>izumi japanese kitchen</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"## üé≠ Sentiment Engine - Advanced Analysis with Fallback","metadata":{}},{"cell_type":"code","source":"class SentimentEngine:\n    \"\"\"Production sentiment analyzer with fallback mechanisms\"\"\"\n    \n    def __init__(self, config: SystemConfig):\n        self.config = config\n        self.model = None\n        self.fallback_mode = False\n        self._load_model()\n    \n    def _load_model(self):\n        \"\"\"Load sentiment model with retry logic\"\"\"\n        try:\n            logger.info(f\"Loading sentiment model: {self.config.sentiment_model}\")\n            self.model = pipeline(\n                \"sentiment-analysis\",\n                model=self.config.sentiment_model,\n                device=0 if self.config.device == \"cuda\" else -1\n            )\n            logger.info(\"‚úÖ Sentiment model loaded\")\n        except Exception as e:\n            logger.warning(f\"Model loading failed: {e}. Enabling fallback mode.\")\n            self.fallback_mode = True\n    \n    def _normalize_label(self, label: str) -> str:\n        \"\"\"Normalize sentiment labels to standard format\"\"\"\n        label_lower = label.lower()\n        if 'pos' in label_lower:\n            return 'positive'\n        elif 'neg' in label_lower:\n            return 'negative'\n        else:\n            return 'neutral'\n    \n    def _fallback_sentiment(self, text: str) -> Tuple[str, float]:\n        \"\"\"Rule-based fallback sentiment analysis\"\"\"\n        text_lower = text.lower()\n        \n        positive_words = ['good', 'great', 'excellent', 'amazing', 'love', 'best', 'wonderful']\n        negative_words = ['bad', 'terrible', 'awful', 'worst', 'hate', 'poor', 'disappointing']\n        \n        pos_count = sum(1 for word in positive_words if word in text_lower)\n        neg_count = sum(1 for word in negative_words if word in text_lower)\n        \n        if pos_count > neg_count:\n            return 'positive', 0.6\n        elif neg_count > pos_count:\n            return 'negative', 0.6\n        else:\n            return 'neutral', 0.5\n    \n    def analyze(self, text: str) -> Tuple[str, float]:\n        \"\"\"Analyze sentiment with error handling\"\"\"\n        try:\n            if self.fallback_mode or self.model is None:\n                return self._fallback_sentiment(text)\n            \n            # Truncate to model limit\n            result = self.model(text[:512])[0]\n            label = self._normalize_label(result['label'])\n            confidence = result['score']\n            \n            return label, confidence\n            \n        except Exception as e:\n            logger.warning(f\"Sentiment analysis failed for text, using fallback: {e}\")\n            return self._fallback_sentiment(text)\n    \n    def batch_analyze(self, texts: List[str]) -> pd.DataFrame:\n        \"\"\"Batch process with progress tracking\"\"\"\n        logger.info(f\"Analyzing {len(texts)} reviews\")\n        \n        results = []\n        for text in tqdm(texts, desc=\"Sentiment Analysis\"):\n            label, confidence = self.analyze(text)\n            results.append({'sentiment': label, 'confidence': confidence})\n        \n        return pd.DataFrame(results)\n\n# Run sentiment analysis\nsentiment_engine = SentimentEngine(config)\nsentiment_results = sentiment_engine.batch_analyze(df_cleaned['review_text'].tolist())\n\ndf_cleaned['overall_sentiment'] = sentiment_results['sentiment']\ndf_cleaned['sentiment_confidence'] = sentiment_results['confidence']\n\nprint(\"\\nüé≠ Sentiment Distribution:\")\nprint(df_cleaned['overall_sentiment'].value_counts())\nprint(f\"\\nAverage Confidence: {df_cleaned['sentiment_confidence'].mean():.2%}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T16:00:28.582523Z","iopub.execute_input":"2026-02-15T16:00:28.582833Z","iopub.status.idle":"2026-02-15T16:00:49.088616Z","shell.execute_reply.started":"2026-02-15T16:00:28.582807Z","shell.execute_reply":"2026-02-15T16:00:49.088079Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/841 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"297bb9dee0cf4cb987183c04bece964a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/1.11G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e84e37b2a99740ef8da960c4b253587d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.11G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e84cac9c6e9848c3ab4deea0e2c51755"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"138c640d93f0477da34639d39dffdeaf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/150 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"545386ea837140fca33cd16018a2938a"}},"metadata":{}},{"name":"stderr","text":"Device set to use cuda:0\nSentiment Analysis:   0%|          | 4/977 [00:00<02:09,  7.53it/s]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\nSentiment Analysis: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 977/977 [00:10<00:00, 96.86it/s] ","output_type":"stream"},{"name":"stdout","text":"\nüé≠ Sentiment Distribution:\noverall_sentiment\npositive    726\nnegative    213\nneutral      38\nName: count, dtype: int64\n\nAverage Confidence: 72.86%\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"## üîç Aspect Extraction - Independent Analysis (NOT Copied from Overall)","metadata":{}},{"cell_type":"code","source":"# ==============================\n# Aspect Extraction Module\n# ==============================\n\nclass AspectAnalyzer:\n    \"\"\"Advanced aspect-based sentiment analyzer (Production Safe)\"\"\"\n\n    def __init__(self, config: SystemConfig, sentiment_engine: SentimentEngine):\n        self.config = config\n        self.sentiment_engine = sentiment_engine\n        self.aspects = config.aspects\n\n    def extract_aspect_text(self, text: str, aspect: str) -> Optional[str]:\n        if not isinstance(text, str) or not text.strip():\n            return None\n\n        text_lower = text.lower()\n        keywords = self.aspects.get(aspect, [])\n\n        # Aspect not mentioned at all\n        if not any(keyword in text_lower for keyword in keywords):\n            return None\n\n        # Extract relevant sentences\n        sentences = text.split(\".\")\n        relevant = []\n\n        for sentence in sentences:\n            if any(keyword in sentence.lower() for keyword in keywords):\n                relevant.append(sentence.strip())\n\n        return \" \".join(relevant) if relevant else text[:200]\n\n    def analyze_aspect(self, text: str, aspect: str) -> Dict[str, Any]:\n        aspect_text = self.extract_aspect_text(text, aspect)\n\n        if aspect_text is None:\n            return {\n                \"mentioned\": False,\n                \"sentiment\": None,\n                \"confidence\": 0.0\n            }\n\n        sentiment, confidence = self.sentiment_engine.analyze(aspect_text)\n\n        return {\n            \"mentioned\": True,\n            \"sentiment\": sentiment,\n            \"confidence\": confidence\n        }\n\n    def batch_analyze(self, texts: List[str]) -> pd.DataFrame:\n        results = []\n\n        for text in tqdm(texts, desc=\"Aspect Analysis\"):\n            row = {}\n\n            for aspect in self.aspects.keys():\n                data = self.analyze_aspect(text, aspect)\n\n                row[f\"{aspect}_mentioned\"] = bool(data[\"mentioned\"])\n                row[f\"{aspect}_sentiment\"] = data[\"sentiment\"]\n                row[f\"{aspect}_confidence\"] = float(data[\"confidence\"])\n\n            results.append(row)\n\n        return pd.DataFrame(results)\n\n\n# ==============================\n# Run Aspect Analysis\n# ==============================\n\naspect_analyzer = AspectAnalyzer(config, sentiment_engine)\n\naspect_results = aspect_analyzer.batch_analyze(\n    df_cleaned[\"review_text\"].fillna(\"\").tolist()\n)\n\n# ------------------------------\n# Remove previous aspect columns (if re-run)\n# ------------------------------\n\naspect_prefixes = list(config.aspects.keys())\n\ncols_to_drop = [\n    col for col in df_cleaned.columns\n    if any(col.startswith(prefix) for prefix in aspect_prefixes)\n]\n\ndf_cleaned = df_cleaned.drop(columns=cols_to_drop, errors=\"ignore\")\n\n# ------------------------------\n# Safe Merge\n# ------------------------------\n\ndf_cleaned = pd.concat(\n    [\n        df_cleaned.reset_index(drop=True),\n        aspect_results.reset_index(drop=True)\n    ],\n    axis=1\n)\n\n# ==============================\n# Safe Aspect Summary\n# ==============================\n\nprint(\"\\nüîç Aspect Analysis Summary:\")\n\nfor aspect in config.aspects.keys():\n\n    col_mentioned = f\"{aspect}_mentioned\"\n    col_sentiment = f\"{aspect}_sentiment\"\n\n    if col_mentioned not in df_cleaned.columns:\n        continue\n\n    mask = df_cleaned[col_mentioned] == True\n    mentioned_count = int(mask.sum())\n\n    if mentioned_count > 0:\n\n        sentiment_dist = (\n            df_cleaned.loc[mask, col_sentiment]\n            .dropna()\n            .value_counts()\n        )\n\n        print(f\"\\n{aspect.upper()} ‚Üí {mentioned_count} mentions\")\n        print(sentiment_dist)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T16:01:08.643986Z","iopub.execute_input":"2026-02-15T16:01:08.644755Z","iopub.status.idle":"2026-02-15T16:01:26.402791Z","shell.execute_reply.started":"2026-02-15T16:01:08.644726Z","shell.execute_reply":"2026-02-15T16:01:26.402089Z"}},"outputs":[{"name":"stderr","text":"Aspect Analysis: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 977/977 [00:17<00:00, 55.11it/s]","output_type":"stream"},{"name":"stdout","text":"\nüîç Aspect Analysis Summary:\n\nFOOD ‚Üí 837 mentions\nfood_sentiment\npositive    595\nnegative    177\nneutral      65\nName: count, dtype: int64\n\nSERVICE ‚Üí 508 mentions\nservice_sentiment\npositive    400\nnegative     88\nneutral      20\nName: count, dtype: int64\n\nPRICE ‚Üí 306 mentions\nprice_sentiment\npositive    130\nnegative    113\nneutral      63\nName: count, dtype: int64\n\nAMBIENCE ‚Üí 388 mentions\nambience_sentiment\npositive    328\nnegative     41\nneutral      19\nName: count, dtype: int64\n\nCLEANLINESS ‚Üí 74 mentions\ncleanliness_sentiment\npositive    55\nnegative    13\nneutral      6\nName: count, dtype: int64\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"## ‚ö†Ô∏è Conflict Detection - Multi-Level Analysis","metadata":{}},{"cell_type":"code","source":"class ConflictDetector:\n    \"\"\"Advanced conflict detection system\"\"\"\n    \n    def detect_rating_sentiment_conflict(self, row: pd.Series) -> str:\n        rating = row.get('review_rating', 0)\n        sentiment = row.get('overall_sentiment')\n        \n        if rating >= 4 and sentiment == 'negative':\n            return 'Hidden Dissatisfaction'\n        elif rating <= 2 and sentiment == 'positive':\n            return 'Politeness Bias'\n        elif rating == 3 and sentiment in ['positive', 'negative']:\n            return 'Ambiguous Experience'\n        else:\n            return 'No Conflict'\n    \n    def detect_aspect_conflicts(self, row: pd.Series, aspects: List[str]) -> int:\n        overall = row.get('overall_sentiment')\n        conflicts = 0\n        \n        for aspect in aspects:\n            if row.get(f'{aspect}_mentioned', False):\n                aspect_sent = row.get(f'{aspect}_sentiment')\n                if aspect_sent and aspect_sent != overall:\n                    conflicts += 1\n        \n        return conflicts\n    \n    def analyze(self, df: pd.DataFrame, aspects: List[str]) -> pd.DataFrame:\n        logger.info(\"Running conflict detection\")\n        \n        df['rating_sentiment_conflict'] = df.apply(\n            self.detect_rating_sentiment_conflict, axis=1\n        )\n        \n        df['aspect_conflict_count'] = df.apply(\n            lambda row: self.detect_aspect_conflicts(row, aspects), axis=1\n        )\n        \n        df['has_conflict'] = (\n            (df['rating_sentiment_conflict'] != 'No Conflict') |\n            (df['aspect_conflict_count'] > 0)\n        )\n        \n        logger.info(\"‚úÖ Conflict detection complete\")\n        return df\n\nconflict_detector = ConflictDetector()\ndf_cleaned = conflict_detector.analyze(df_cleaned, list(config.aspects.keys()))\n\nprint(\"\\n‚ö†Ô∏è Conflict Analysis:\")\nprint(df_cleaned['rating_sentiment_conflict'].value_counts())\nprint(f\"\\nTotal Conflicts: {df_cleaned['has_conflict'].sum():,} ({df_cleaned['has_conflict'].mean():.1%})\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T16:01:36.239022Z","iopub.execute_input":"2026-02-15T16:01:36.239352Z","iopub.status.idle":"2026-02-15T16:01:36.276247Z","shell.execute_reply.started":"2026-02-15T16:01:36.239327Z","shell.execute_reply":"2026-02-15T16:01:36.275533Z"}},"outputs":[{"name":"stdout","text":"\n‚ö†Ô∏è Conflict Analysis:\nrating_sentiment_conflict\nNo Conflict               799\nAmbiguous Experience       87\nHidden Dissatisfaction     82\nPoliteness Bias             9\nName: count, dtype: int64\n\nTotal Conflicts: 364 (37.3%)\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"## Text Chunking for RAG","metadata":{}},{"cell_type":"code","source":"from langchain.text_splitter import RecursiveCharacterTextSplitter\n\nsplitter = RecursiveCharacterTextSplitter(\n    chunk_size=400,\n    chunk_overlap=50\n)\n\ndef chunk_reviews(df):\n    new_rows = []\n\n    for _, row in df.iterrows():\n        chunks = splitter.split_text(row[\"review_text\"])\n\n        for chunk in chunks:\n            new_row = row.copy()\n            new_row[\"review_text\"] = chunk\n            new_rows.append(new_row)\n\n    return pd.DataFrame(new_rows)\n\ndf_chunked = chunk_reviews(df_cleaned)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T16:01:42.229381Z","iopub.execute_input":"2026-02-15T16:01:42.230224Z","iopub.status.idle":"2026-02-15T16:01:42.419411Z","shell.execute_reply.started":"2026-02-15T16:01:42.230177Z","shell.execute_reply":"2026-02-15T16:01:42.418802Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"## üóÑÔ∏è Vector Database - RAG Layer","metadata":{}},{"cell_type":"code","source":"import shutil\n\nclass VectorStoreManager:\n    \"\"\"Production vector database manager\"\"\"\n    \n    def __init__(self, config: SystemConfig):\n        self.config = config\n        self.embeddings = None\n        self.vector_store = None\n        self._initialize_embeddings()\n    \n    def _initialize_embeddings(self):\n        try:\n            logger.info(f\"Loading embedding model: {self.config.embedding_model}\")\n            self.embeddings = HuggingFaceEmbeddings(\n                model_name=self.config.embedding_model\n            )\n            logger.info(\"‚úÖ Embedding model loaded\")\n        except Exception as e:\n            logger.error(f\"Embedding model loading failed: {e}\")\n            raise\n    \n    def create_documents(self, df: pd.DataFrame) -> List[Document]:\n        logger.info(f\"Creating {len(df)} documents\")\n        documents = []\n        \n        for _, row in df.iterrows():\n            aspect_summary = {}\n            for aspect in self.config.aspects.keys():\n                if row.get(f'{aspect}_mentioned', False):\n                    aspect_summary[aspect] = row.get(f'{aspect}_sentiment')\n            \n            doc = Document(\n                page_content=row['review_text'],\n                metadata={\n                    'restaurant': row['business_name'],\n                    'restaurant_normalized': row['business_name_normalized'],\n                    'rating': float(row['review_rating']),\n                    'sentiment': row['overall_sentiment'],\n                    'confidence': float(row['sentiment_confidence']),\n                    'aspects': json.dumps(aspect_summary),\n                    'conflict': row['rating_sentiment_conflict'],\n                    'has_conflict': bool(row['has_conflict'])\n                }\n            )\n            documents.append(doc)\n        \n        return documents\n    \n    def build_vector_store(self, documents: List[Document]) -> Chroma:\n        try:\n            if os.path.exists(self.config.vector_db_path):\n                shutil.rmtree(self.config.vector_db_path)\n            \n            logger.info(\"Building vector store...\")\n            self.vector_store = Chroma.from_documents(\n                documents=documents,\n                embedding=self.embeddings,\n                persist_directory=self.config.vector_db_path,\n                client_settings=Settings(anonymized_telemetry=False)\n            )\n            \n            logger.info(\"‚úÖ Vector store built\")\n            return self.vector_store\n        except Exception as e:\n            logger.error(f\"Vector store creation failed: {e}\")\n            raise\n\nvector_manager = VectorStoreManager(config)\ndocuments = vector_manager.create_documents(df_chunked)\nvector_store = vector_manager.build_vector_store(documents)\n\nprint(f\"\\nüóÑÔ∏è Vector Store Ready - {len(documents):,} documents\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T16:01:48.998454Z","iopub.execute_input":"2026-02-15T16:01:48.999134Z","iopub.status.idle":"2026-02-15T16:01:53.921975Z","shell.execute_reply.started":"2026-02-15T16:01:48.999104Z","shell.execute_reply":"2026-02-15T16:01:53.921149Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e3b6d9972744352a472f9dbd37b81ee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a31e89ebd3cc439b8b0fae458aa640f5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b5df32125e77448d95b19b803961d280"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b3509f234af4b1096847c337342f0fa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"294db8ec4efb4c67917b3d53ed24f10a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"118ed445e6f64e6c9275c8c4ccbb815c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da308ce21d3449fab3eb2b02dafcd5c5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a6c84ce7b9c7456a862e9ca301faa551"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3311899af8ec48648fc1c7a7bf60b70f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db8dff4a5f9d4b839d349c00c9bb189a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17128a99ea4949d999eafeb9c9661a43"}},"metadata":{}},{"name":"stdout","text":"\nüóÑÔ∏è Vector Store Ready - 1,541 documents\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"### CELL 9: LLM Provider Layer ###","metadata":{}},{"cell_type":"code","source":"import google.generativeai as genai\nimport os\n\n# API key configure\ngenai.configure(api_key=\"AIzaSyAGvjHfkxwniulHPIvCE7k5QvZKdscBixA\")\n\nclass GeminiLLM:\n    def __init__(self, temperature=0.0):\n        self.model = genai.GenerativeModel(\"models/gemini-2.5-flash\")\n        self.temperature = temperature\n\n    def __call__(self, prompt: str):\n        response = self.model.generate_content(\n            prompt,\n            generation_config={\n                \"temperature\": 0.0,\n                \"top_p\": 0.8\n            }\n        )\n        return response.text\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T16:41:33.272817Z","iopub.execute_input":"2026-02-15T16:41:33.273557Z","iopub.status.idle":"2026-02-15T16:41:33.278456Z","shell.execute_reply.started":"2026-02-15T16:41:33.273525Z","shell.execute_reply":"2026-02-15T16:41:33.277679Z"}},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":"### CELL 10: RAG Chatbot ###","metadata":{}},{"cell_type":"code","source":"import re\nimport numpy as np\n\nclass RAGChatbot:\n\n    def __init__(self, vector_store, llm, config, df):\n        self.vector_store = vector_store\n        self.llm = llm\n        self.config = config\n        self.df = df\n        self.memory = []\n        self.recommendation_engine = RecommendationEngine(df)\n\n    # ---------------------------\n    # Intent Detection\n    # ---------------------------\n    def classify_query(self, question):\n        q = question.lower()\n\n        if re.search(r\"top\\s*\\d+\", q):\n            return \"top_n\"\n\n        if \"compare\" in q:\n            return \"comparison\"\n\n        if \"best\" in q or \"rank\" in q:\n            return \"ranking\"\n\n        if \"recommend\" in q:\n            return \"recommendation\"\n\n        return \"general\"\n\n    # ---------------------------\n    # Extract Top N\n    # ---------------------------\n    def extract_top_n(self, question):\n        match = re.search(r\"top\\s*(\\d+)\", question.lower())\n        if match:\n            return int(match.group(1))\n        return None\n\n    # ---------------------------\n    # Extract Restaurant Entities\n    # ---------------------------\n    def extract_entities(self, question):\n        q = question.lower()\n        entities = []\n\n        for name in self.df[\"business_name_normalized\"].unique():\n            if name in q:\n                entities.append(name)\n\n        return entities\n\n    # ---------------------------\n    # Dataset-wide Ranking\n    # ---------------------------\n    def rank_restaurants(self, top_n=5, focus=\"overall\"):\n\n        results = []\n\n        for r in self.df[\"business_name_normalized\"].unique():\n            score_data = self.recommendation_engine.calculate_score(r)\n\n            if \"error\" not in score_data:\n                results.append(score_data)\n\n        ranked = sorted(results, key=lambda x: x[\"total_score\"], reverse=True)\n\n        return ranked[:top_n]\n\n    # ---------------------------\n    # Multi-Restaurant Comparison\n    # ---------------------------\n    def compare_restaurants(self, entities):\n\n        comparisons = []\n\n        for e in entities:\n            score_data = self.recommendation_engine.calculate_score(e)\n            if \"error\" not in score_data:\n                comparisons.append(score_data)\n\n        if len(comparisons) < 2:\n            return None\n\n        ranked = sorted(comparisons, key=lambda x: x[\"total_score\"], reverse=True)\n\n        return ranked\n\n    # ---------------------------\n    # Human-Friendly Presentation Layer\n    # ---------------------------\n    def present_ranking(self, ranked_list):\n\n        text = \"Here are the strongest performers based on your dataset:\\n\\n\"\n\n        for i, r in enumerate(ranked_list, 1):\n            text += f\"{i}. {r['restaurant']} (Score: {r['total_score']})\\n\"\n\n        text += \"\\nThese rankings are calculated using sentiment strength, rating consistency, volume, and confidence.\"\n\n        return text\n\n    def present_comparison(self, ranked_list):\n\n        text = \"Here‚Äôs a clear comparison based on your dataset:\\n\\n\"\n\n        for r in ranked_list:\n            text += (\n                f\"{r['restaurant']} ‚Üí \"\n                f\"Score: {r['total_score']}, \"\n                f\"Avg Rating: {r['average_rating']}, \"\n                f\"Reviews: {r['review_count']}\\n\"\n            )\n\n        winner = ranked_list[0][\"restaurant\"]\n\n        text += f\"\\nOverall, {winner} currently shows the strongest performance based on available data.\"\n\n        return text\n\n    # ---------------------------\n    # General RAG Mode\n    # ---------------------------\n    def rag_answer(self, question):\n\n        docs = self.vector_store.similarity_search(question, k=self.config.retrieval_k)\n\n        if not docs:\n            return {\n                \"answer\": \"I don‚Äôt see enough relevant review evidence for that specific question.\",\n                \"confidence\": 0.0\n            }\n\n        context = \"\\n\".join([doc.page_content for doc in docs])\n\n        prompt = f\"\"\"\nYou are a restaurant analytics consultant.\n\nAnswer naturally and clearly.\nAvoid academic tone.\nUse only provided evidence.\n\nEvidence:\n{context}\n\nQuestion:\n{question}\n\"\"\"\n\n        answer = self.llm(prompt)\n\n        return {\n            \"answer\": answer,\n            \"confidence\": 0.85\n        }\n\n    # ---------------------------\n    # Main Query Router\n    # ---------------------------\n    def query(self, question, restaurant_filter=None):\n\n        intent = self.classify_query(question)\n\n        # -------- Top N Mode --------\n        if intent == \"top_n\":\n            top_n = self.extract_top_n(question) or 5\n            ranked = self.rank_restaurants(top_n)\n            answer = self.present_ranking(ranked)\n\n            return {\n                \"answer\": answer,\n                \"confidence\": 0.95,\n                \"intent\": intent\n            }\n\n        # -------- Comparison Mode --------\n        entities = self.extract_entities(question)\n\n        if intent == \"comparison\" and len(entities) >= 2:\n            ranked = self.compare_restaurants(entities)\n\n            if ranked:\n                answer = self.present_comparison(ranked)\n\n                return {\n                    \"answer\": answer,\n                    \"confidence\": 0.93,\n                    \"intent\": intent\n                }\n\n        # -------- General RAG Mode --------\n        rag_response = self.rag_answer(question)\n        rag_response[\"intent\"] = \"general\"\n\n        return rag_response\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T16:41:37.431296Z","iopub.execute_input":"2026-02-15T16:41:37.431847Z","iopub.status.idle":"2026-02-15T16:41:37.445946Z","shell.execute_reply.started":"2026-02-15T16:41:37.431822Z","shell.execute_reply":"2026-02-15T16:41:37.445182Z"}},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":"### CELL 11: Recommendation Engine ###","metadata":{}},{"cell_type":"code","source":"class RecommendationEngine:\n    def __init__(self, df: pd.DataFrame):\n        self.df = df\n\n    def calculate_score(self, restaurant_name: str):\n\n        df_rest = self.df[\n            self.df[\"business_name_normalized\"] == restaurant_name.lower()\n        ]\n\n        if df_rest.empty:\n            return {\"error\": \"Restaurant not found\"}\n\n        if len(df_rest) < 5:\n            return {\"error\": \"Not enough review volume for reliable ranking\"}\n\n        # =============================\n        # Sentiment Score (40%)\n        # =============================\n        sentiment_score = (df_rest[\"overall_sentiment\"] == \"positive\").mean() * 40\n\n        # =============================\n        # Aspect Score (25%)\n        # =============================\n        aspect_cols = [c for c in df_rest.columns if \"_sentiment\" in c]\n\n        aspect_score_total = 0\n        aspect_count = 0\n\n        for col in aspect_cols:\n            mentioned = df_rest[df_rest[col.replace(\"_sentiment\",\"_mentioned\")] == True]\n            if not mentioned.empty:\n                pos_rate = (mentioned[col] == \"positive\").mean()\n                aspect_score_total += pos_rate\n                aspect_count += 1\n\n        aspect_score = (aspect_score_total / aspect_count) * 25 if aspect_count > 0 else 0\n\n        # =============================\n        # Conflict Penalty (15%)\n        # =============================\n        conflict_rate = df_rest[\"has_conflict\"].mean()\n        conflict_score = (1 - conflict_rate) * 15\n\n        # =============================\n        # Volume Bonus (10%)\n        # =============================\n        volume_score = min(len(df_rest) / 100, 1.0) * 10\n\n        # =============================\n        # Confidence Bonus (10%)\n        # =============================\n        confidence_score = df_rest[\"sentiment_confidence\"].mean() * 10\n\n        total_score = (\n            sentiment_score +\n            aspect_score +\n            conflict_score +\n            volume_score +\n            confidence_score\n        )\n\n        return {\n            \"restaurant\": restaurant_name,\n            \"total_score\": round(total_score, 2),\n            \"review_volume\": len(df_rest),\n            \"sentiment_positive_ratio\": round((df_rest[\"overall_sentiment\"] == \"positive\").mean(), 2),\n            \"conflict_rate\": round(conflict_rate, 2)\n        }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T16:41:44.195479Z","iopub.execute_input":"2026-02-15T16:41:44.195798Z","iopub.status.idle":"2026-02-15T16:41:44.203640Z","shell.execute_reply.started":"2026-02-15T16:41:44.195752Z","shell.execute_reply":"2026-02-15T16:41:44.203039Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"llm = GeminiLLM()\nchatbot = RAGChatbot(vector_store, llm, config, df_cleaned)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T16:41:51.083043Z","iopub.execute_input":"2026-02-15T16:41:51.083410Z","iopub.status.idle":"2026-02-15T16:41:51.087139Z","shell.execute_reply.started":"2026-02-15T16:41:51.083384Z","shell.execute_reply":"2026-02-15T16:41:51.086467Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"# --- Ensure Chatbot Initialized ---\n\nif \"vector_store\" not in globals():\n    raise RuntimeError(\"‚ùå Vector store not found. Run vector store build cell first.\")\n\nif \"llm\" not in globals():\n    llm = GeminiLLM()\n\nif \"chatbot\" not in globals():\n    if \"df_cleaned\" in globals():\n        chatbot = RAGChatbot(vector_store, llm, config, df_cleaned)\n    elif \"df_chunked\" in globals():\n        chatbot = RAGChatbot(vector_store, llm, config, df_chunked)\n    else:\n        raise RuntimeError(\"‚ùå No dataframe found for chatbot initialization.\")\n    \n    print(\"‚úÖ Chatbot auto-initialized successfully\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T16:02:19.015647Z","iopub.execute_input":"2026-02-15T16:02:19.016143Z","iopub.status.idle":"2026-02-15T16:02:19.021014Z","shell.execute_reply.started":"2026-02-15T16:02:19.016116Z","shell.execute_reply":"2026-02-15T16:02:19.020194Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Chatbot auto-initialized successfully\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"### CELL 12: Interactive Chat Interface ###","metadata":{}},{"cell_type":"code","source":"from IPython.display import display, HTML, clear_output\nimport ipywidgets as widgets\n\n# Chat history\nchat_history = []\n\n# Create UI components\noutput_area = widgets.Output()\nquestion_input = widgets.Text(\n    placeholder='Ask about restaurants (e.g., \"Best food quality?\")',\n    description='Question:',\n    layout=widgets.Layout(width='70%')\n)\nrestaurant_input = widgets.Text(\n    placeholder='Optional: Filter by restaurant name',\n    description='Restaurant:',\n    layout=widgets.Layout(width='70%')\n)\nsend_button = widgets.Button(\n    description='Send',\n    button_style='primary',\n    icon='paper-plane'\n)\nclear_button = widgets.Button(\n    description='Clear Chat',\n    button_style='warning',\n    icon='trash'\n)\n\ndef format_chat_message(role, message, sources=None):\n    \"\"\"Format chat message with styling\"\"\"\n    if role == \"user\":\n        return f'''\n        <div style=\"background: #e3f2fd; padding: 10px; margin: 5px 0; border-radius: 10px; border-left: 4px solid #2196F3;\">\n            <strong>üßë You:</strong> {message}\n        </div>\n        '''\n    else:\n        return f\"\"\"\n        <div style='background: #f5f5f5; padding: 10px; margin: 5px; border-radius: 10px; border-left: 4px solid #6C63FF;'>\n            <strong>ü§ñ Assistant:</strong><br>\n            {message}\n        </div>\n        \"\"\"\n\ndef send_message(b):\n    \"\"\"Handle send button click\"\"\"\n    question = question_input.value.strip()\n    restaurant = restaurant_input.value.strip() if restaurant_input.value.strip() else None\n\n    if not question:\n        with output_area:\n            clear_output(wait=True)\n            for msg in chat_history:\n                display(HTML(msg))\n            display(HTML('<p style=\"color: red;\">‚ö†Ô∏è Please enter a qquestion</p>'))\n        return\n\n    # Add user message to history\n    user_msg = format_chat_message(\"user\", question + (f\" (Restaurant: {restaurant})\" if restaurant else \"\"))\n    chat_history.append(user_msg)\n\n    # Get response\n    result = chatbot.query(question, restaurant_filter=restaurant)  \n\n    # Add bot response to history\n    bot_msg = format_chat_message(\n        \"assistant\",\n        result[\"answer\"] + f\"\\n\\nConfidence: {result['confidence']}\"\n    )\n\n    chat_history.append(bot_msg)\n\n    # Update display\n    with output_area:\n        clear_output(wait=True)\n        for msg in chat_history:\n            display(HTML(msg))\n\n    # Clear inputs\n    question_input.value = \"\"\n    restaurant_input.value = \"\"\n\ndef clear_chat(b):\n    \"\"\"Clear chat history\"\"\"\n    global chat_history\n    chat_history = []\n    with output_area:\n        clear_output()\n        display(HTML('<p style=\"color: #666;\">Chat cleared. Start a new conversation!</p>'))\n\n# Attach event handlers\nsend_button.on_click(send_message)\nclear_button.on_click(clear_chat)\nquestion_input.on_submit(lambda x: send_message(None))\n\n# Display UI\nprint(\"\\n\" + \"=\"*80)\nprint(\"ü§ñ INTERACTIVE RESTAURANT CHATBOT\")\nprint(\"=\"*80)\nprint(\"\\nAsk questions about restaurants or get recommendations!\")  \nprint(\"\\nExample questions:\")\nprint(\"  - Which restaurant has the best food?\")\nprint(\"  - Tell me about the service quality\")\nprint(\"  - Is Izumi Japanese Kitchen good for couples?\")\nprint(\"  - What do people say about prices?\")\nprint(\"\\n\" + \"=\"*80 + \"\\n\")\n\ndisplay(widgets.VBox([\n    widgets.HTML(\"<h3>üçΩÔ∏è Restaurant Intelligence Chatbot</h3>\"),    \n    question_input,\n    restaurant_input,\n    widgets.HBox([send_button, clear_button]),\n    output_area\n]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T16:41:55.866708Z","iopub.execute_input":"2026-02-15T16:41:55.867302Z","iopub.status.idle":"2026-02-15T16:41:55.891656Z","shell.execute_reply.started":"2026-02-15T16:41:55.867276Z","shell.execute_reply":"2026-02-15T16:41:55.890765Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nü§ñ INTERACTIVE RESTAURANT CHATBOT\n================================================================================\n\nAsk questions about restaurants or get recommendations!\n\nExample questions:\n  - Which restaurant has the best food?\n  - Tell me about the service quality\n  - Is Izumi Japanese Kitchen good for couples?\n  - What do people say about prices?\n\n================================================================================\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<h3>üçΩÔ∏è Restaurant Intelligence Chatbot</h3>'), Text(value='', description='Question‚Ä¶","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5be9a09f41ed4d9a98ff105a023d300f"}},"metadata":{}}],"execution_count":33},{"cell_type":"code","source":"docs = vector_store.similarity_search(\"KFC\", k=3)\n\nfor d in docs:\n    print(d.metadata)\n    print(\"------\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"entities = chatbot.extract_entities(\"Tell me about Pallabi Hotel & Restaurant\")\n\ndocs = chatbot.hybrid_retrieval(\n    \"Tell me about Pallabi Hotel & Restaurant\",\n    entities,\n    k=6\n)\n\nprint(\"Total docs:\", len(docs))\n\nfor d in docs:\n    print(d.metadata[\"restaurant\"])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# # # Data Export for FastAPI  ","metadata":{}},{"cell_type":"code","source":"import os\nimport shutil\n\n# Create data directory\ndata_dir = \"data\"\nos.makedirs(data_dir, exist_ok=True)\n\n# Export processed DataFrame to CSV\noutput_path = os.path.join(data_dir, \"processed_reviews.csv\")\ndf_cleaned.to_csv(output_path, index=False)\nprint(f\"‚úÖ Exported {len(df_cleaned)} reviews to {output_path}\")\n\n# Export as pickle (faster loading)\npickle_path = os.path.join(data_dir, \"processed_reviews.pkl\")\ndf_cleaned.to_pickle(pickle_path)\nprint(f\"‚úÖ Exported pickle to {pickle_path}\")\n\n# Copy vector database\nsource_vector_db = \"/kaggle/working/restaurant_vector_db\"\ntarget_vector_db = \"restaurant_vector_db\"\n\nif os.path.exists(source_vector_db):\n    if os.path.exists(target_vector_db):\n        shutil.rmtree(target_vector_db)\n    shutil.copytree(source_vector_db, target_vector_db)\n    print(f\"‚úÖ Copied vector database to {target_vector_db}\")\nelse:\n    print(\"‚ö†Ô∏è Vector database not found\")\n\nprint(\"\\nüéâ Data export complete!\")\nprint(\"\\nFiles created:\")\nprint(\"  - data/processed_reviews.csv\")\nprint(\"  - data/processed_reviews.pkl\")\nprint(\"  - restaurant_vector_db/ (folder)\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport shutil\nfrom langchain.vectorstores import Chroma\nfrom chromadb.config import Settings\n\n# Target Path\nVECT_DB_PATH = \"/kaggle/working/restaurant_vector_db\"\n\nprint(f\"üéØ Force building at: {VECT_DB_PATH}\")\n\n# 1. Clean up\nif os.path.exists(VECT_DB_PATH):\n    shutil.rmtree(VECT_DB_PATH)\n\n# 2. Get documents and embeddings manually\nprint(\"üìÑ Preparing documents...\")\nvector_manager = VectorStoreManager(config)\ndocuments = vector_manager.create_documents(df_chunked)\nembeddings = vector_manager.embeddings\n\n# 3. Direct Chroma Creation (Bypassing Manager)\nprint(\"üèóÔ∏è Creating ChromaDB directly...\")\nvector_store = Chroma.from_documents(\n    documents=documents,\n    embedding=embeddings,\n    persist_directory=VECT_DB_PATH,\n    client_settings=Settings(anonymized_telemetry=False)\n)\n\n# 4. Force Persist (IMPORTANT for older Chroma versions)\ntry:\n    vector_store.persist()\n    print(\"üíæ Forced persistence called.\")\nexcept:\n    print(\"‚ÑπÔ∏è Auto-persistence enabled (newer Chroma version).\")\n\n# 5. Verify & Zip\nif os.path.exists(VECT_DB_PATH):\n    print(f\"‚úÖ SUCCESS! Folder created: {os.listdir(VECT_DB_PATH)}\")\n    shutil.make_archive(\"restaurant_vector_db\", 'zip', root_dir=\"/kaggle/working\", base_dir=\"restaurant_vector_db\")\n    print(\"üéâ 'restaurant_vector_db.zip' is ready to download!\")\nelse:\n    print(\"‚ùå STILL FAILED. ChromaDB is refusing to write to this path.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}